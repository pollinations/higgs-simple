#!/usr/bin/env python3
"""
Simple Audio Service
OpenAI-compatible audio API with text-to-speech and speech-to-text.
Fails fast if dependencies are missing - no conditional logic.
"""

import os
import base64
import tempfile
import logging
import io
import time
from typing import Optional
from flask import Flask, request, jsonify, Response
from flask_cors import CORS

# Required imports - fail fast if not available
from boson_multimodal.serve.serve_engine import HiggsAudioServeEngine
from boson_multimodal.data_types import ChatMLSample, Message, AudioContent
import whisper
import torch
import torchaudio

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Global model instances
tts_model = None
stt_model = None

def load_models():
    """Load models - fail fast if any dependency is missing"""
    global tts_model, stt_model
    
    logger.info("Loading TTS model...")
    tts_model = HiggsAudioServeEngine(
        "bosonai/higgs-audio-v2-generation-3B-base", 
        "bosonai/higgs-audio-v2-tokenizer"
    )
    logger.info("TTS model loaded successfully")
    
    logger.info("Loading STT model...")
    stt_model = whisper.load_model("small")
    logger.info("STT model loaded successfully")

def decode_base64_audio(b64_string: str) -> bytes:
    """Decode base64 audio data"""
    try:
        # Remove data URL prefix if present
        if b64_string.startswith('data:audio'):
            b64_string = b64_string.split(',')[1]
        
        # Fix padding if needed
        missing_padding = len(b64_string) % 4
        if missing_padding:
            b64_string += '=' * (4 - missing_padding)
        
        return base64.b64decode(b64_string)
    except Exception as e:
        raise ValueError(f"Invalid base64 audio data: {e}")

def text_to_speech(text: str) -> bytes:
    """Convert text to speech"""
    try:
        # Create simple chat template for TTS
        system_prompt = (
            "You are a voice synthesis engine. Speak the user's text naturally and expressively. "
            "Generate audio following instruction.\n"
            "<|scene_desc_start|>\n"
            "Speak with natural conversational warmth and genuine human connection. "
            "Use a balanced, expressive voice with organic pacing and authentic emotional undertones.\n"
            "<|scene_desc_end|>"
        )
        
        messages = [
            Message(role="system", content=system_prompt),
            Message(role="user", content=text)
        ]
        
        # Create chat template
        chat_template = ChatMLSample(messages=messages)
        
        # Generate audio
        response = tts_model.generate(
            chat_ml_sample=chat_template,
            max_new_tokens=1024,
            temperature=0.7,
            force_audio_gen=True,
            top_k=50,
            top_p=0.95
        )
        
        if response.audio is None:
            raise RuntimeError("No audio generated by model")
        
        # Convert to WAV bytes
        audio_tensor = torch.from_numpy(response.audio).unsqueeze(0)
        sample_rate = getattr(response, 'sampling_rate', 24000)
        
        buffer = io.BytesIO()
        torchaudio.save(buffer, audio_tensor, sample_rate, format="WAV")
        audio_bytes = buffer.getvalue()
        
        logger.info(f"Generated audio: {len(audio_bytes)} bytes at {sample_rate}Hz")
        return audio_bytes
        
    except Exception as e:
        logger.error(f"TTS error: {e}")
        raise RuntimeError(f"Text-to-speech failed: {e}")

def speech_to_text(audio_bytes: bytes) -> str:
    """Convert speech to text"""
    try:
        # Save audio to temporary file
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
            temp_file.write(audio_bytes)
            temp_audio_path = temp_file.name
        
        try:
            # Transcribe audio
            result = stt_model.transcribe(temp_audio_path)
            transcription = result["text"].strip()
            logger.info(f"Transcribed: {transcription}")
            return transcription
        finally:
            # Clean up temp file
            if os.path.exists(temp_audio_path):
                os.unlink(temp_audio_path)
                
    except Exception as e:
        logger.error(f"STT error: {e}")
        raise RuntimeError(f"Speech-to-text failed: {e}")

@app.route("/health", methods=["GET"])
def health():
    """Health endpoint"""
    return jsonify({"status": "ok"})

@app.route("/v1/chat/completions", methods=["POST"])
def chat_completions():
    """OpenAI Chat Completions API compatible endpoint with multimodal audio support"""
    try:
        data = request.get_json(force=True)
        
        # Extract parameters
        messages = data.get("messages", [])
        model = data.get("model", "gpt-4o-audio-preview")
        modalities = data.get("modalities", ["text"])
        audio_config = data.get("audio", {})
        
        if not messages:
            return jsonify({"error": {"message": "Missing required 'messages' parameter", "type": "invalid_request_error"}}), 400
        
        # Process the conversation
        response_text = ""
        response_audio = None
        
        # Extract the last user message
        last_message = messages[-1] if messages else None
        if not last_message or last_message.get("role") != "user":
            return jsonify({"error": {"message": "Last message must be from user", "type": "invalid_request_error"}}), 400
        
        # Handle different content types
        content = last_message.get("content")
        if isinstance(content, str):
            # Simple text input
            response_text = f"I understand you said: {content}"
        elif isinstance(content, list):
            # Multimodal content
            text_parts = []
            audio_input = None
            
            for item in content:
                if item.get("type") == "text":
                    text_parts.append(item.get("text", ""))
                elif item.get("type") == "input_audio":
                    # Handle audio input
                    audio_data = item.get("input_audio", {}).get("data")
                    if audio_data:
                        try:
                            audio_bytes = base64.b64decode(audio_data)
                            transcription = speech_to_text(audio_bytes)
                            text_parts.append(f"[Audio transcription: {transcription}]")
                            audio_input = audio_bytes
                        except Exception as e:
                            logger.error(f"Audio processing error: {e}")
                            text_parts.append("[Audio processing failed]")
            
            response_text = f"I processed your message: {' '.join(text_parts)}"
        
        # Generate response
        response_message = {
            "role": "assistant",
            "content": response_text
        }
        
        # Generate audio if requested
        if "audio" in modalities:
            try:
                audio_bytes = text_to_speech(response_text)
                audio_b64 = base64.b64encode(audio_bytes).decode('utf-8')
                response_message["audio"] = {
                    "data": audio_b64,
                    "format": audio_config.get("format", "wav")
                }
            except Exception as e:
                logger.error(f"Audio generation error: {e}")
                # Continue without audio if generation fails
        
        # Return OpenAI-compatible response
        return jsonify({
            "id": f"chatcmpl-{os.urandom(16).hex()}",
            "object": "chat.completion",
            "created": int(os.path.getmtime(__file__)),
            "model": model,
            "choices": [{
                "index": 0,
                "message": response_message,
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": len(str(messages)),
                "completion_tokens": len(response_text),
                "total_tokens": len(str(messages)) + len(response_text)
            }
        })
            
    except Exception as e:
        logger.error(f"Chat completion error: {e}")
        return jsonify({"error": {"message": str(e), "type": "server_error"}}), 500

@app.errorhandler(404)
def not_found(e):
    return jsonify({"error": "Endpoint not found"}), 404

@app.errorhandler(500)
def internal_error(e):
    return jsonify({"error": "Internal server error"}), 500

if __name__ == "__main__":
    logger.info("Starting Simple Audio Service...")
    
    # Load models on startup
    load_models()
    
    # Start server
    host = "0.0.0.0"
    port = 8000
    logger.info(f"Server starting on {host}:{port}")
    
    app.run(host=host, port=port, debug=False, threaded=True)
