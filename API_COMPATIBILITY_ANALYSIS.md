# API Compatibility Analysis: audio.pollinations vs Current Implementation

## üîç **Original audio.pollinations Voice Cloning API**

### API Structure
The original service uses a specialized message format for voice cloning:

```json
{
  "messages": [
    {
      "role": "user", 
      "content": [
        {
          "type": "text",
          "text": "Say hello in my voice"
        },
        {
          "type": "voice",
          "voice": {
            "name": "alloy",
            "data": "<base64_audio_string>", 
            "format": "wav"
          }
        },
        {
          "type": "clone_audio_transcript",
          "audio_text": "Transcription or description of the reference audio"
        }
      ]
    }
  ]
}
```

### Key Features
- **Explicit Voice Reference**: Uses `"type": "voice"` content type
- **Voice Metadata**: Includes voice name, data, and format
- **Optional Transcript**: `clone_audio_transcript` for voice description
- **Dual Architecture**: Separate model server (port 8001) + API server (port 8000)
- **Multiple Endpoints**: `/audio` (GET/POST) + internal `/synthesize`, `/transcribe`

## üéØ **Current Implementation Analysis**

### What We Have
- **OpenAI Compatibility**: `/v1/chat/completions` endpoint
- **Standard Audio Input**: Uses `"type": "input_audio"` (OpenAI format)
- **Higgs Audio Engine**: Same underlying TTS model as original
- **Simple Architecture**: Single-process design
- **Base64 Audio**: Working input/output audio handling

### What's Missing for Voice Cloning
- **Voice Reference Detection**: No logic to extract voice from input audio
- **Clone-Specific Processing**: No differentiation between transcription vs voice cloning
- **Voice Token Integration**: Not passing voice reference to generation model

## üîÑ **API Design Options**

### Option 1: Maintain Pure OpenAI Compatibility
```json
{
  "model": "gpt-4o-audio-preview",
  "modalities": ["text", "audio"],
  "audio": {"voice": "clone", "format": "wav"},
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "Say this in my voice"},
        {"type": "input_audio", "input_audio": {"data": "base64_voice_sample"}}
      ]
    }
  ]
}
```

**Pros**: 
- ‚úÖ Pure OpenAI compatibility
- ‚úÖ Clean API design
- ‚úÖ Easy to understand

**Cons**:
- ‚ùå Less explicit about voice cloning intent
- ‚ùå Harder to distinguish transcription vs voice cloning

### Option 2: Hybrid Approach (Recommended)
Support both OpenAI format AND original pollinations format:

```json
// OpenAI Style (auto-detect voice cloning)
{
  "audio": {"voice": "clone"},
  "messages": [{"role": "user", "content": [
    {"type": "text", "text": "Say hello"},
    {"type": "input_audio", "input_audio": {"data": "..."}}
  ]}]
}

// Pollinations Style (explicit voice cloning)
{
  "messages": [{"role": "user", "content": [
    {"type": "text", "text": "Say hello"},
    {"type": "voice", "voice": {"data": "...", "format": "wav"}}
  ]}]
}
```

**Pros**:
- ‚úÖ Maximum compatibility
- ‚úÖ Explicit voice cloning support
- ‚úÖ Backward compatibility with existing clients

**Cons**:
- ‚ö†Ô∏è Slightly more complex implementation

### Option 3: Pure Pollinations Compatibility
Implement exact original API structure.

**Pros**:
- ‚úÖ Drop-in replacement for original service
- ‚úÖ Explicit voice cloning semantics

**Cons**:
- ‚ùå Breaks OpenAI compatibility
- ‚ùå More complex message parsing

## üéØ **Recommended Implementation Strategy**

### **Hybrid Approach** (Option 2)

1. **Maintain OpenAI Compatibility**: Keep existing `/v1/chat/completions` endpoint
2. **Add Voice Detection Logic**: 
   - If `audio.voice == "clone"` ‚Üí use input_audio as voice reference
   - If content contains `"type": "voice"` ‚Üí explicit voice cloning
3. **Graceful Fallback**: Default to standard TTS if voice cloning fails

### Implementation Priority
1. **Phase 1**: OpenAI-style voice cloning (`audio.voice: "clone"`)
2. **Phase 2**: Add pollinations-style content type support
3. **Phase 3**: Optional transcript support

## üõ†Ô∏è **Technical Implementation Notes**

### Voice Reference Processing
```python
def extract_voice_reference(content_items):
    """Extract voice reference from message content"""
    for item in content_items:
        # OpenAI style
        if item.get("type") == "input_audio" and should_clone_voice():
            return item.get("input_audio", {}).get("data")
        # Pollinations style  
        elif item.get("type") == "voice":
            return item.get("voice", {}).get("data")
    return None

def should_clone_voice():
    """Check if voice cloning is requested via audio config"""
    return request.json.get("audio", {}).get("voice") == "clone"
```

### Model Integration
The Higgs Audio model already supports voice cloning - we just need to:
1. Convert voice reference audio to tokens
2. Include voice tokens in ChatML sample
3. Configure generation for voice cloning mode

## üìä **Compatibility Matrix**

| Feature | Original | Current | Recommended |
|---------|----------|---------|-------------|
| OpenAI `/v1/chat/completions` | ‚ùå | ‚úÖ | ‚úÖ |
| Pollinations `/audio` | ‚úÖ | ‚ùå | ‚ö†Ô∏è (Optional) |
| `"type": "voice"` content | ‚úÖ | ‚ùå | ‚úÖ |
| `"type": "input_audio"` | ‚ùå | ‚úÖ | ‚úÖ |
| `audio.voice: "clone"` | ‚ùå | ‚ùå | ‚úÖ |
| Voice transcripts | ‚úÖ | ‚ùå | ‚ö†Ô∏è (Future) |
| Single process | ‚ùå | ‚úÖ | ‚úÖ |

## üöÄ **Next Steps**

1. **Update VOICE_CLONING_PLAN.md** with hybrid approach
2. **Implement OpenAI-style voice cloning first** (simpler, maintains compatibility)
3. **Add pollinations content type support** (for maximum compatibility)
4. **Test with both API formats** to ensure compatibility

---

**Recommendation**: Start with **OpenAI-style voice cloning** (`audio.voice: "clone"`) as it's simpler to implement and maintains full compatibility with existing OpenAI clients, then add pollinations-style support for maximum compatibility.
